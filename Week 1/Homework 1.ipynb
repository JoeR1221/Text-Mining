{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "from nltk.collocations import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source of .txt files: https://www.gutenberg.org/ebooks/42671 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process files for text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load .txt files\n",
    "SS = open(\"Sense_and_Sensibility.txt\", \"r\")\n",
    "PP = open(\"Pride_and_Prejudice.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS = SS.read()\n",
    "PP = PP.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "SS_tokens = nltk.word_tokenize(SS)\n",
    "PP_tokens = nltk.word_tokenize(PP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144488\n",
      "144561\n"
     ]
    }
   ],
   "source": [
    "#View length of tokens\n",
    "print(len(SS_tokens))\n",
    "print(len(PP_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'which', 'we', 'have', 'referred', '.', '_Pride', 'and', 'Prejudice_', 'it', 'is', 'true', ',', 'was', 'written', 'and', 'finished', 'before', '_Sense', 'and', 'Sensibility_', '--', 'its', 'original', 'title', 'for', 'several', 'years', 'being', '_First', 'Impressions_', '.', 'Then', ',', 'in', '1797', ',', 'the', 'author', 'fell', 'to', 'work', 'upon', 'an', 'older', 'essay', 'in', 'letters', '_Ãƒ', 'la_']\n"
     ]
    }
   ],
   "source": [
    "#Viiew sample of tokens\n",
    "print(SS_tokens[150:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all letters to lowercase to allow for proper cleansing\n",
    "def lowercase(text):\n",
    "    token_list = []\n",
    "    for word in text:\n",
    "        word_lower = word.lower()\n",
    "        token_list.append(word_lower)\n",
    "    return(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_tokens = lowercase(SS_tokens)\n",
    "PP_tokens = lowercase(PP_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "#Add extra stopwords in addition to the default nltk stopwords list\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "more_words = [\"mr.\",\"mrs.\", \"miss\", \"sir\", \"lady\", \"colonel\", \"de\", \"chapter\"]\n",
    "stop_words = nltkstopwords + more_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'mr.',\n",
       " 'mrs.',\n",
       " 'miss',\n",
       " 'sir',\n",
       " 'lady',\n",
       " 'colonel',\n",
       " 'de',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_stop = [word for word in SS_tokens if word not in stop_words]\n",
    "PP_stop = [word for word in PP_tokens if word not in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76109\n",
      "75797\n"
     ]
    }
   ],
   "source": [
    "#View length after removal of stopwords\n",
    "print(len(SS_stop))\n",
    "print(len(PP_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming using Lancaster method\n",
    "Porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_stem = [Porter.stem(t) for t in SS_stop]\n",
    "PP_stem = [Porter.stem(t) for t in PP_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non-letter characters\n",
    "SS_clean = [re.sub(r'[^a-zA-Z0-9]+', '', word) for word in SS_stem]\n",
    "SS_final = list(filter(None, SS_clean))\n",
    "\n",
    "PP_clean = [re.sub(r'[^a-zA-Z0-9]+', '', word) for word in PP_stem]\n",
    "PP_final = list(filter(None, PP_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54824\n",
      "54328\n"
     ]
    }
   ],
   "source": [
    "#View length after removal of stopwords\n",
    "print(len(SS_final))\n",
    "print(len(PP_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View top 50 most common words for both <i> Sense and Sensibility </i> and <i> Pride and Prejudice </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 50 most common words in Sense and Sensibility are:\n",
      "\n",
      "[('elinor', 693), ('s', 660), ('could', 582), ('mariann', 569), ('would', 520), ('said', 399), ('everi', 377), ('one', 346), ('sister', 320), ('dashwood', 294), ('much', 291), ('must', 289), ('know', 273), ('edward', 267), ('time', 263), ('mother', 253), ('think', 238), ('jen', 236), ('well', 224), ('see', 218), ('though', 217), ('might', 217), ('willoughbi', 214), ('say', 213), ('thing', 208), ('never', 191), ('day', 190), ('noth', 189), ('luci', 186), ('may', 184), ('even', 182), ('soon', 181), ('without', 176), ('wish', 172), ('first', 170), ('ever', 170), ('feel', 169), ('make', 169), ('littl', 167), ('look', 166), ('give', 166), ('john', 166), ('happi', 166), ('go', 162), ('two', 157), ('howev', 157), ('good', 157), ('hous', 155), ('great', 154), ('thought', 151)]\n"
     ]
    }
   ],
   "source": [
    "SS_top_50 = nltk.FreqDist(SS_final)\n",
    "PP_top_50 = nltk.FreqDist(PP_final)\n",
    "\n",
    "# Print the 50 most common tokens in The Dead...\n",
    "print(\"The 50 most common words in Sense and Sensibility are:\\n\")\n",
    "print(SS_top_50.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 50 most common words in Pride and Prejudice are:\n",
      "\n",
      "[('elizabeth', 634), ('s', 596), ('could', 523), ('would', 470), ('darci', 409), ('said', 402), ('bennet', 334), ('much', 328), ('bingley', 312), ('must', 308), ('one', 305), ('jane', 292), ('sister', 292), ('everi', 285), ('know', 273), ('think', 237), ('though', 226), ('time', 222), ('never', 221), ('well', 219), ('soon', 218), ('see', 212), ('say', 210), ('make', 206), ('good', 203), ('might', 200), ('may', 194), ('wickham', 194), ('thing', 191), ('littl', 187), ('wish', 183), ('noth', 178), ('collin', 178), ('look', 174), ('lydia', 171), ('come', 170), ('without', 170), ('hope', 168), ('feel', 167), ('friend', 167), ('day', 165), ('shall', 163), ('go', 161), ('even', 160), ('dear', 158), ('famili', 157), ('like', 157), ('give', 157), ('happi', 155), ('man', 149)]\n"
     ]
    }
   ],
   "source": [
    "# Print the 50 most common tokens in The Dead...\n",
    "print(\"The 50 most common words in Pride and Prejudice are:\\n\")\n",
    "print(PP_top_50.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View top 50 most common bigrams for both <i> Sense and Sensibility </i> and <i> Pride and Prejudice </i> using frequency score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "SS_bigrams = BigramCollocationFinder.from_words(SS_final)\n",
    "SS_bigrams.apply_ngram_filter(lambda w1, w2: len(w2) < 2)\n",
    "\n",
    "SS_bigrams_top_50 = SS_bigrams.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 50 bigrams in Sense and Sensibility by frequency score are:\n",
      "\n",
      "(('everi', 'thing'), 0.0014592149423610099)\n",
      "(('said', 'elinor'), 0.0011856121406683205)\n",
      "(('john', 'dashwood'), 0.0006931270976214797)\n",
      "(('dare', 'say'), 0.0006566467240624544)\n",
      "(('everi', 'bodi'), 0.0006201663505034291)\n",
      "(('said', 'mariann'), 0.0005472056033853787)\n",
      "(('thousand', 'pound'), 0.0005472056033853787)\n",
      "(('elinor', 'could'), 0.0005289654166058661)\n",
      "(('everi', 'day'), 0.0005107252298263534)\n",
      "(('repli', 'elinor'), 0.0004924850430468408)\n",
      "(('young', 'man'), 0.00047424485626732817)\n",
      "(('cri', 'mariann'), 0.00045600466948781553)\n",
      "(('great', 'deal'), 0.00043776448270830295)\n",
      "(('would', 'give'), 0.00034656354881073984)\n",
      "(('berkeley', 'street'), 0.00031008317525171457)\n",
      "(('next', 'morn'), 0.00031008317525171457)\n",
      "(('day', 'two'), 0.00029184298847220193)\n",
      "(('harley', 'street'), 0.00029184298847220193)\n",
      "(('next', 'day'), 0.00029184298847220193)\n",
      "(('said', 'jen'), 0.00029184298847220193)\n",
      "(('sens', 'sensibility'), 0.00029184298847220193)\n",
      "(('two', 'thousand'), 0.00029184298847220193)\n",
      "(('young', 'ladi'), 0.00029184298847220193)\n",
      "(('edward', 'ferrar'), 0.00027360280169268935)\n",
      "(('pride', 'prejudice'), 0.00027360280169268935)\n",
      "(('soon', 'afterward'), 0.00027360280169268935)\n",
      "(('almost', 'everi'), 0.0002553626149131767)\n",
      "(('noth', 'could'), 0.0002553626149131767)\n",
      "(('put', 'end'), 0.0002553626149131767)\n",
      "(('could', 'make'), 0.00023712242813366408)\n",
      "(('john', 'middleton'), 0.00023712242813366408)\n",
      "(('poor', 'edward'), 0.00023712242813366408)\n",
      "(('said', 'dashwood'), 0.00023712242813366408)\n",
      "(('sure', 'would'), 0.00023712242813366408)\n",
      "(('took', 'place'), 0.00023712242813366408)\n",
      "(('two', 'three'), 0.00023712242813366408)\n",
      "(('wo', 'nt'), 0.00023712242813366408)\n",
      "(('barton', 'park'), 0.00021888224135415148)\n",
      "(('elinor', 'mariann'), 0.00021888224135415148)\n",
      "(('elinor', 'would'), 0.00021888224135415148)\n",
      "(('half', 'hour'), 0.00021888224135415148)\n",
      "(('last', 'night'), 0.00021888224135415148)\n",
      "(('short', 'time'), 0.00021888224135415148)\n",
      "(('sure', 'must'), 0.00021888224135415148)\n",
      "(('would', 'never'), 0.00021888224135415148)\n",
      "(('comb', 'magna'), 0.00020064205457463884)\n",
      "(('could', 'hardli'), 0.00020064205457463884)\n",
      "(('could', 'help'), 0.00020064205457463884)\n",
      "(('elinor', 'said'), 0.00020064205457463884)\n",
      "(('ever', 'sinc'), 0.00020064205457463884)\n"
     ]
    }
   ],
   "source": [
    "print(\"The top 50 bigrams in Sense and Sensibility by frequency score are:\\n\")\n",
    "for bscore in SS_bigrams_top_50[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "PP_bigrams = BigramCollocationFinder.from_words(PP_final)\n",
    "PP_bigrams.apply_ngram_filter(lambda w1, w2: len(w2) < 2)\n",
    "\n",
    "PP_bigrams_top_50 = PP_bigrams.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 50 bigrams in Pride and Prejudice by frequency score are:\n",
      "\n",
      "(('everi', 'thing'), 0.0010859961714033278)\n",
      "(('said', 'elizabeth'), 0.0008467088793992049)\n",
      "(('young', 'man'), 0.0006994551612428214)\n",
      "(('everi', 'bodi'), 0.0006442350169341776)\n",
      "(('dare', 'say'), 0.0005706081578559859)\n",
      "(('said', 'bennet'), 0.0004969812987777941)\n",
      "(('elizabeth', 'could'), 0.0004785745840082462)\n",
      "(('young', 'ladi'), 0.00046016786923869826)\n",
      "(('great', 'deal'), 0.00044176115446915035)\n",
      "(('uncl', 'aunt'), 0.00044176115446915035)\n",
      "(('cri', 'elizabeth'), 0.00042335443969960244)\n",
      "(('went', 'away'), 0.00038654101016050656)\n",
      "(('could', 'help'), 0.0003681342953909586)\n",
      "(('half', 'hour'), 0.0003681342953909586)\n",
      "(('repli', 'elizabeth'), 0.0003681342953909586)\n",
      "(('thousand', 'pound'), 0.0003681342953909586)\n",
      "(('next', 'morn'), 0.0003497275806214107)\n",
      "(('good', 'humour'), 0.0003129141510823148)\n",
      "(('made', 'answer'), 0.0003129141510823148)\n",
      "(('mr', 'darci'), 0.0003129141510823148)\n",
      "(('much', 'better'), 0.0003129141510823148)\n",
      "(('dear', 'lizzi'), 0.0002945074363127669)\n",
      "(('said', 'bingley'), 0.0002945074363127669)\n",
      "(('soon', 'afterward'), 0.0002945074363127669)\n",
      "(('everi', 'day'), 0.000276100721543219)\n",
      "(('let', 'us'), 0.000276100721543219)\n",
      "(('next', 'day'), 0.000276100721543219)\n",
      "(('whole', 'parti'), 0.000276100721543219)\n",
      "(('catherin', 'bourgh'), 0.000257694006773671)\n",
      "(('could', 'think'), 0.000257694006773671)\n",
      "(('depend', 'upon'), 0.000257694006773671)\n",
      "(('elizabeth', 'felt'), 0.000257694006773671)\n",
      "(('said', 'darci'), 0.000257694006773671)\n",
      "(('said', 'jane'), 0.000257694006773671)\n",
      "(('would', 'never'), 0.000257694006773671)\n",
      "(('oh', 'dear'), 0.0002392872920041231)\n",
      "(('two', 'three'), 0.0002392872920041231)\n",
      "(('william', 'luca'), 0.0002392872920041231)\n",
      "(('come', 'back'), 0.00022088057723457517)\n",
      "(('could', 'hardli'), 0.00022088057723457517)\n",
      "(('jane', 'elizabeth'), 0.00022088057723457517)\n",
      "(('oh', 'ye'), 0.00022088057723457517)\n",
      "(('think', 'ill'), 0.00022088057723457517)\n",
      "(('would', 'give'), 0.00022088057723457517)\n",
      "(('elizabeth', 'soon'), 0.00020247386246502724)\n",
      "(('enter', 'room'), 0.00020247386246502724)\n",
      "(('never', 'heard'), 0.00020247386246502724)\n",
      "(('never', 'saw'), 0.00020247386246502724)\n",
      "(('noth', 'could'), 0.00020247386246502724)\n",
      "(('take', 'place'), 0.00020247386246502724)\n"
     ]
    }
   ],
   "source": [
    "print(\"The top 50 bigrams in Pride and Prejudice by frequency score are:\\n\")\n",
    "for bscore in PP_bigrams_top_50[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View top 50 most common bigrams for both <i> Sense and Sensibility </i> and <i> Pride and Prejudice </i> using mutual information and a minimum frequency of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 50 bigrams in Sense and Sensibility by MI are:\n",
      "\n",
      "(('15th', 'october'), 15.72940831698402)\n",
      "(('adept', 'scienc'), 15.72940831698402)\n",
      "(('although', 'utterli'), 15.72940831698402)\n",
      "(('anywher', 'else'), 15.72940831698402)\n",
      "(('blenheim', 'warwick'), 15.72940831698402)\n",
      "(('bounti', 'benefic'), 15.72940831698402)\n",
      "(('bowl', 'punch'), 15.72940831698402)\n",
      "(('briberi', 'corrupt'), 15.72940831698402)\n",
      "(('broadfac', 'stuffi'), 15.72940831698402)\n",
      "(('bromley', 'course'), 15.72940831698402)\n",
      "(('buri', 'parishion'), 15.72940831698402)\n",
      "(('cap', 'powder'), 15.72940831698402)\n",
      "(('caper', 'frisk'), 15.72940831698402)\n",
      "(('cleans', 'impur'), 15.72940831698402)\n",
      "(('coax', 'threaten'), 15.72940831698402)\n",
      "(('commonest', 'dullest'), 15.72940831698402)\n",
      "(('conniv', 'aid'), 15.72940831698402)\n",
      "(('deem', 'indispens'), 15.72940831698402)\n",
      "(('default', 'heir'), 15.72940831698402)\n",
      "(('deter', 'foul'), 15.72940831698402)\n",
      "(('dishonest', 'dishonesti'), 15.72940831698402)\n",
      "(('dove', 'dale'), 15.72940831698402)\n",
      "(('droop', 'apac'), 15.72940831698402)\n",
      "(('east', 'bourn'), 15.72940831698402)\n",
      "(('eclat', 'proverb'), 15.72940831698402)\n",
      "(('enforc', 'earnestly'), 15.72940831698402)\n",
      "(('fat', 'haunch'), 15.72940831698402)\n",
      "(('forbad', 'disclosur'), 15.72940831698402)\n",
      "(('gaudi', 'uselessli'), 15.72940831698402)\n",
      "(('gift', 'godfath'), 15.72940831698402)\n",
      "(('grape', 'nectarin'), 15.72940831698402)\n",
      "(('gulf', 'impass'), 15.72940831698402)\n",
      "(('ham', 'chicken'), 15.72940831698402)\n",
      "(('heavier', 'chagrin'), 15.72940831698402)\n",
      "(('heir', 'male'), 15.72940831698402)\n",
      "(('illiter', 'miserli'), 15.72940831698402)\n",
      "(('impel', 'unqualifi'), 15.72940831698402)\n",
      "(('incumbr', 'mysteri'), 15.72940831698402)\n",
      "(('inflex', 'studiou'), 15.72940831698402)\n",
      "(('invect', 'villan'), 15.72940831698402)\n",
      "(('irreligi', 'immor'), 15.72940831698402)\n",
      "(('kenelworth', 'birmingham'), 15.72940831698402)\n",
      "(('largest', 'folio'), 15.72940831698402)\n",
      "(('laurel', 'hedg'), 15.72940831698402)\n",
      "(('m', 'tallest'), 15.72940831698402)\n",
      "(('minc', 'pie'), 15.72940831698402)\n",
      "(('minds', 'unsoci'), 15.72940831698402)\n",
      "(('multitud', 'connubi'), 15.72940831698402)\n",
      "(('nectarin', 'peach'), 15.72940831698402)\n",
      "(('oak', 'spanish'), 15.72940831698402)\n"
     ]
    }
   ],
   "source": [
    "SS_bigrams.apply_freq_filter(5)\n",
    "SS_bigrams = PP_bigrams.score_ngrams(bigram_measures.pmi)\n",
    "print(\"The top 50 bigrams in Sense and Sensibility by MI are:\\n\")\n",
    "for bscore in SS_bigrams[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 50 bigrams in Pride and Prejudice by MI are:\n",
      "\n",
      "(('st', 'jame'), 12.922053394926415)\n",
      "(('humbl', 'abod'), 11.243981489813779)\n",
      "(('thousand', 'pound'), 10.378911069899887)\n",
      "(('shook', 'head'), 10.18251385709638)\n",
      "(('beg', 'pardon'), 9.922053394926413)\n",
      "(('lift', 'eye'), 9.663319126526247)\n",
      "(('gone', 'scotland'), 9.360174507318298)\n",
      "(('ten', 'thousand'), 9.013914253120637)\n",
      "(('card', 'tabl'), 8.922053394926415)\n",
      "(('younger', 'son'), 8.871427321856448)\n",
      "(('insist', 'upon'), 8.863159705872846)\n",
      "(('drew', 'near'), 8.518006679565548)\n",
      "(('step', 'forward'), 8.483380336455916)\n",
      "(('fair', 'cousin'), 8.363086102738201)\n",
      "(('luca', 'lodg'), 8.257733102591974)\n",
      "(('put', 'end'), 8.237555220654347)\n",
      "(('depend', 'upon'), 8.122078003234407)\n",
      "(('great', 'deal'), 8.056982975012522)\n",
      "(('make', 'hast'), 8.042907789800802)\n",
      "(('half', 'hour'), 8.028968598842928)\n",
      "(('short', 'paus'), 7.948048603459361)\n",
      "(('georg', 'wickham'), 7.9368503968544974)\n",
      "(('year', 'ago'), 7.905586183337647)\n",
      "(('uncl', 'aunt'), 7.775696864622507)\n",
      "(('william', 'luca'), 7.756538959563418)\n",
      "(('fall', 'love'), 7.752128393484103)\n",
      "(('dearest', 'lizzi'), 7.64194547573368)\n",
      "(('ten', 'minut'), 7.583051786680111)\n",
      "(('thousand', 'year'), 7.5185630602284)\n",
      "(('situat', 'life'), 7.504200880040514)\n",
      "(('next', 'morn'), 7.467560826100668)\n",
      "(('larg', 'parti'), 7.440311614564029)\n",
      "(('went', 'stair'), 7.4002847206924525)\n",
      "(('run', 'away'), 7.399611978763314)\n",
      "(('good', 'graciou'), 7.386000494686206)\n",
      "(('five', 'minut'), 7.363086102738205)\n",
      "(('finish', 'letter'), 7.363086102738203)\n",
      "(('good', 'humour'), 7.3441803189915795)\n",
      "(('catherin', 'bourgh'), 7.297164709792499)\n",
      "(('open', 'door'), 7.249628052954918)\n",
      "(('whole', 'parti'), 7.245055323165094)\n",
      "(('dare', 'say'), 7.1879993961801105)\n",
      "(('went', 'away'), 7.185271829721602)\n",
      "(('mention', 'name'), 7.179211234423542)\n",
      "(('three', 'month'), 7.167165892762947)\n",
      "(('young', 'ladi'), 7.089163380761674)\n",
      "(('either', 'side'), 7.062493958677615)\n",
      "(('young', 'men'), 7.046094658869789)\n",
      "(('long', 'ago'), 7.040283912070759)\n",
      "(('longbourn', 'estat'), 7.0234304153014975)\n"
     ]
    }
   ],
   "source": [
    "PP_bigrams.apply_freq_filter(5)\n",
    "PP_bigrams = PP_bigrams.score_ngrams(bigram_measures.pmi)\n",
    "print(\"The top 50 bigrams in Pride and Prejudice by MI are:\\n\")\n",
    "for bscore in PP_bigrams[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondaf3202b1bf8894605bdcf88f1af902d88"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
